[
    {
        "name": "ViT_batch10",
        "dataset_name": [
            "https://www.dropbox.com/scl/fi/30dlbblp7wytcvoy05col/report_train.npy?rlkey=jx100qoz5n1d654v2mi32i9aj&dl=1",
            "https://www.dropbox.com/scl/fi/oo6g1yqhbjm22wffeddgv/report_val.npy?rlkey=9rqe0rucjhrbzv3x7xbs5047z&dl=1"
        ],
        "train_losses": [
            1.6237958699464798,
            0.9197404913604259,
            0.6098153049126267,
            0.5390470463782548,
            0.4951177814975381,
            0.43959041237831115,
            0.46110482662916186,
            0.42026778729632497,
            0.3517277600243688,
            0.3225815535522997,
            0.2966148553416133,
            0.3039799249265343,
            0.3244876435957849,
            0.2860191332176328,
            0.27523766255471854,
            0.284434668533504,
            0.2582087494898587,
            0.22983051352202893,
            0.17540750089101492,
            0.2324993339832872,
            0.2022064710734412,
            0.19515344686806202,
            0.3377506537362933,
            0.1338797241449356,
            0.19533505991566927,
            0.17072526562260465,
            0.18856010909657925,
            0.20679698484018444,
            0.10165258084889502,
            0.12128047117730603
        ],
        "val_losses": [
            1.3082891200718128,
            0.7033056373658934,
            0.6180027942908438,
            0.5729973394619791,
            0.5912413346140009,
            0.6402904595199385,
            0.49808929390028905,
            0.4950888870578063,
            0.5667009828122038,
            0.5265387777043017,
            0.5747973769903183,
            0.5185409552956882,
            0.571050627451194,
            0.6254362518850126,
            0.607401636478148,
            0.6416341780047667,
            0.6267227997121058,
            0.5228107830411509,
            0.8037385548415937,
            0.601322917561782,
            0.5760121180822975,
            0.6389831053583246,
            0.5609362776342192,
            0.5480515576506916,
            0.6108417699211522,
            0.5539511662760848,
            0.5870381976036649,
            0.5431728747330213,
            0.6570049685082937,
            0.6374993238009905
        ],
        "train_accuracies": [
            30.11764705882353,
            62.07843137254902,
            76.7843137254902,
            79.17647058823529,
            81.05882352941177,
            83.17647058823529,
            81.13725490196079,
            84.3529411764706,
            86.50980392156863,
            87.92156862745098,
            89.2156862745098,
            89.05882352941177,
            87.80392156862744,
            89.88235294117646,
            89.52941176470588,
            88.98039215686275,
            89.88235294117646,
            91.09803921568627,
            93.72549019607843,
            91.13725490196079,
            92.54901960784314,
            92.98039215686275,
            87.25490196078431,
            95.17647058823529,
            92.54901960784314,
            93.17647058823529,
            92.58823529411765,
            92.58823529411765,
            96.03921568627452,
            95.45098039215686
        ],
        "val_accuracies": [
            43.333333333333336,
            70.66666666666667,
            77.83333333333333,
            77.16666666666667,
            77.0,
            76.16666666666667,
            80.33333333333333,
            79.83333333333333,
            79.66666666666667,
            80.5,
            79.0,
            80.0,
            80.33333333333333,
            76.16666666666667,
            76.83333333333333,
            78.0,
            78.33333333333333,
            82.0,
            77.16666666666667,
            79.0,
            80.83333333333333,
            80.0,
            80.33333333333333,
            83.33333333333333,
            79.33333333333333,
            80.16666666666667,
            81.66666666666667,
            82.66666666666667,
            80.0,
            81.66666666666667
        ],
        "traning_times": [
            "2023-12-03 06:51:33.419853",
            "2023-12-03 06:51:46.397823",
            "2023-12-03 06:51:58.882003",
            "2023-12-03 06:52:11.964684",
            "2023-12-03 06:52:24.845927",
            "2023-12-03 06:52:37.478413",
            "2023-12-03 06:52:50.080277",
            "2023-12-03 06:53:02.484776",
            "2023-12-03 06:53:14.868655",
            "2023-12-03 06:53:27.337888",
            "2023-12-03 06:53:39.802386",
            "2023-12-03 06:53:52.259784",
            "2023-12-03 06:54:04.810014",
            "2023-12-03 06:54:17.320613",
            "2023-12-03 06:54:29.815781",
            "2023-12-03 06:54:42.255638",
            "2023-12-03 06:54:54.693291",
            "2023-12-03 06:55:07.366164",
            "2023-12-03 06:55:19.877319",
            "2023-12-03 06:55:32.364534",
            "2023-12-03 06:55:44.861373",
            "2023-12-03 06:55:57.350302",
            "2023-12-03 06:56:09.850083",
            "2023-12-03 06:56:22.450175",
            "2023-12-03 06:56:34.983551",
            "2023-12-03 06:56:47.482657",
            "2023-12-03 06:56:59.985278",
            "2023-12-03 06:57:12.508933",
            "2023-12-03 06:57:25.017754",
            "2023-12-03 06:57:37.512421"
        ],
        "model": "ViT(\n  (tokenizer): Image2Tokens(\n    (to_patch_embedding): Sequential(\n      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=10, p2=10)\n      (1): Linear(in_features=300, out_features=192, bias=True)\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (transformer): Transformer(\n    (layers): ModuleList(\n      (0-8): 9 x ModuleList(\n        (0): PreNorm(\n          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (fn): Attention(\n            (attend): Softmax(dim=-1)\n            (to_qkv): Linear(in_features=192, out_features=576, bias=True)\n            (to_out): Sequential(\n              (0): Linear(in_features=192, out_features=192, bias=True)\n              (1): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (1): PreNorm(\n          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (fn): FeedForwardNetwork(\n            (net): Sequential(\n              (0): Linear(in_features=192, out_features=768, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n              (3): Linear(in_features=768, out_features=192, bias=True)\n              (4): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    (1): Linear(in_features=192, out_features=6, bias=True)\n  )\n)",
        "test_data": [
            "https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy",
            "https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy"
        ],
        "test_avg_loss": 7.20088329440669,
        "test_accuracy": 18.333333333333332
    }
]